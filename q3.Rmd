This problem focuses on the collinearity problem.

```{r}
library(tidyverse)
set.seed(1)  # make sure everybody will have the same "random" numbers
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + rnorm(100)
collinearity <- tibble(x1 = x1, x2 = x2, y = y)
```

(a) Use the function `cor` to compute the sample correction between `x1` and `x2`.
cor(x1,x2) = 0.83512

(b) Using this data, fit a least squares regression to predict `y` using `x1` and `x2`. Describe the results obtained. How do estimated coefficients relate to the true coefficients?
y = 2.1305 + 1.4396(x1) + 0.7097(x2)
x2 = .5(x1) + rnorm(100)/10 => rnorm(100) = 10(x2)-5(x1), 
y = 2 + 2(x1) + 10(x2) - 5(x1) = 2 - 3(x1) + 10(x2)

(c) Now fit a least squares regression to predict `y` using only `x1`. Comment on your results.
y = 2.1178 + 1.8166(x1)
The intercept decreases, but the slope of the x1 term increased. This means that x1 and x2 should have an inverse relationship given the same y.

d) Which model, part (b) or part (c), give a smaller prediction error? Use a test set to check it.
MSE(x1 and x2 regression) = 0.7568851
MSE(x1 regression) = 0.7618674
Part (b) gives the lowest error
